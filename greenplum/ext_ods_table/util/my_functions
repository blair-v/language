#!/bin/bash
# desc : my commonly used functions

function import_gpdata_from_rds1() {
  if [ $# -ne 3 ] ; then
    echo_ex "args count $# is error!"
    exit 255
  fi
  begin_time=$1
  end_time=$2
  json_file=$3
  python ${datax_home}/bin/datax.py -p "-Dbegin_time='$begin_time' -Dend_time='${end_time}' -Dgpextdata='${gpextdata}' -Dusername='${rds1_username}' -Dpassword='${rds1_password}' -DjdbcUrl='${rds1_jdbcUrl}'" $json_file
}

# ${jdbc_table} ${hive_table}
#function: import jdbc_data to hive_table
function import_data_hive() {
  if [ $# -ne 5 ] ; then
    echo_ex "args count $# is error!"
    exit 255
  fi
  jdbc_table=$1
  hive_table=$2

  jdbc_url=$3
  jdbc_username=$4
  jdbc_passwd=$5
  echo "start import hive. from ${jdbc_table}..."
  if [ -n "${jdbc_table}" ] ; then
    echo_ex "${HADOOP} fs -rmr /user/hdfs/${jdbc_table}"
    ${HADOOP} fs -rmr /user/hdfs/${jdbc_table}
  fi
  sqoop import \
    --connect ${jdbc_url} \
    --username ${jdbc_username} --password  ${jdbc_passwd} \
    --table ${jdbc_table} --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --hive-import --hive-overwrite --hive-table ${hive_table} \
    --null-string '\\N' --null-non-string '\\N'
  check_success
  echo "end successful import to ${hive_table}"
}
# ${jdbc_table} ${hive_dir} ${hive_table} ${sql}
function import_data_query_hive() {
  if [ $# -lt 7 ] ; then
    echo_ex "args count $# is error!"
    exit 255
  fi
  jdbc_table=$1
  hive_dir=$2
  hive_table=$3
  sql=$4

  jdbc_url=$5
  jdbc_username=$6
  jdbc_passwd=$7
  id="id"
  if [ $# -gt 7 ] ; then
    id=$8
  fi
  
  echo_ex "${sql}"
  if [ -n "${hive_table}" ] && [ ${hive_table} != "/" ]; then
    ${HADOOP} fs -rmr "${hive_dir}${hive_table}"
  fi
  sqoop import \
    --connect ${jdbc_url} \
    --username ${jdbc_username} --password ${jdbc_passwd} \
    --query "${sql}" --split-by ${id} -m 10 \
    --target-dir ${hive_dir}/${hive_table} \
    --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --null-string '\\N' --null-non-string '\\N'
}
#Author  : libin
#agrs 3  : jdbc_table, exec_sql, target_dir
#function: import_data_to_hdfs
function import_data_hdfs() {
  if [ $# -lt 5 ] ; then
    echo_ex "args count $# is error!"
    return 255
  fi
  exec_sql=$1
  target_dir=$2

  jdbc_url=$3
  jdbc_username=$4
  jdbc_passwd=$5
  id="id"
  if [ $# -gt 5 ] ; then
    id=$6
  fi
  echo_ex "$exec_sql"
  echo_ex "start import from jdbc_table..."
  if [ -n "${target_dir}" ] && [ ${target_dir} != "/" ]; then
    echo_ex "${HADOOP} fs -rmr ${target_dir}"
    ${HADOOP} fs -rmr "${target_dir}"
  fi
  echo_ex "-split-by ${id} -m 10"
  echo_ex "${target_dir}"
  sqoop import \
    --connect ${jdbc_url} --username ${jdbc_username} --password  ${jdbc_passwd} \
    --query "${exec_sql}" \
    --split-by ${id} -m 10 \
    --target-dir ${target_dir} \
    --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --hive-drop-import-delims \
    --null-string '\\N' --null-non-string '\\N'
  check_success
  echo_ex "end successful import ${target_dir}. field.delim : \001"
}
