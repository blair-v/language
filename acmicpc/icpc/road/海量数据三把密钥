海量数据
 密钥一 ：分而治之/hash映射  +  Hash统计 +  堆/快速/归并排序
1、海量日志数据，提取出某日访问百度次数最多的那个IP。
2、寻找热门查询，300万个查询字符串中统计最热门的10个查询
3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。
   返回频数最高的100个词。
 (1) 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到
     5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有
     的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不
     超过1M。
 (2)hash统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率.
 (3)堆/归并排序： 取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个
        词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并
        （类似于归并排序）的过程了。
4, 海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。
 解法 ： 先把所有的数据遍历做一次hash, (保证相同的数据划分到同一台电脑上进行计算)
         然后根据hash结果重新分布到100台电脑中，接下来自己想。
5, 有10个文件，每个文件 1G， 每个文件的每一行存放的都是用户的query,每个文件的query
   都有可能重复。要求你按照query的频度排序。
 解法 ： 
   hash映射 ：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10
         个文件(记为：a0, a1, a2, a3, ..., a9)中。这样新生成的每个文件的大小也约为
         1G(假设hash函数是随机的)
   hash统计 ：找一台内存在2G 左右的机器，依次对 hash_map(query, query_count)来统计
         每个query出现的次数。
   堆/快速/归并排序：按照出现的次数进行排序，将排好序的query和对应的query_count输出
         到文件，这样得到了10个排好序的文件。最后，对这10个排好序的文件进行归并排序。
6, 给定a，b两个文件，各存放50亿个url,每个url各占64字节，内存限制是4G，让你找出a, b文
   件共同的url.
 分析 ： 
    每个文件的大小大约为 320G，内存装不下。
  (1) 分而治之/hash映射：
       遍历文件a, 对每个url求取，然后根据所取得的值将url分别存储到1000
       个小文件中，这样每个小文件大约为300M。遍历文件b,采取与a相同的方式将url分别存储到1000
       个小文件中。这样处理后，所有可能相同的url都在对应的小文件中，然后只要求出1000对小文件
       中相同的url即可。
 （2）Hash统计。 对每对小文件中相同的url,可以把其中一个小文件的url存到hash_set中。
密钥二 ：双层桶划分
 适用范围  : 第K大，中位数，不重复或重复的数字。
 要点     ： 因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后
             最后在一个可以接受的范围内进行。通过多次缩小，双层只是一个例子。
7、怎么在海量数据中找出重复次数最多的一个？
    方案1：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重
       复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。
8、上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。
   方案1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二
         叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第2
         题提到的堆机制完成。
9、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
     方案1： 这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n*le)
          （le表示单词的平准长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，
           前面的题中已经讲到了，时间复杂度是O(n*lg10)。所以总的时间复杂度，
           是O(n*le)与O(n*lg10)中较大的哪一个。
10. 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？
 方案1： 这题用trie树比较合适，hash_map也行。
 方案2： from xjbzju:，1000w的数据规模插入操作完全不现实，以前试过在stl下100w元素插
        入set中已经慢得不能忍受，觉得基于hash的实现不会比红黑树好太多，使用
        vector+sort+unique都要可行许多，建议还是先hash成小文件分开处理再综合。
 11，2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 
    分析 ： 2*bit * 2^32
 12，5亿个int找它们的中位数。
         分析 ：首先我们将int划分为 2^16 个区域。
密钥三 ：Bitmap
 15, 给40亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否
     在那 40 亿个数当中？
   分析 ： frome oo, 用位图/Bitmap的方法。申请512M内存。
